x-airflow-common: &airflow-common
  build:
    context: docker/
    dockerfile: Dockerfile
  env_file:
    - .env
  environment:
    AIRFLOW__CORE__EXECUTOR: LocalExecutor 
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}    
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "false"
    AIRFLOW__CORE__DEFAULT_TIMEZONE: Asia/Seoul

    # .env data 
    BRONZE_BUCKET: ${BRONZE_BUCKET}
    SILVER_BUCKET: ${SILVER_BUCKET}
    GOLD_BUCKET: ${GOLD_BUCKET}

    # file path and name
    DATA_BASE_PREFIX_SEJU: ${DATA_BASE_PREFIX_SEJU}
    DATA_FINAL_PREFIX_SEJU: ${DATA_FINAL_PREFIX_SEJU}
    DATA_BASE_PREFIX_DREAM: ${DATA_BASE_PREFIX_DREAM}
    DATA_FINAL_PREFIX_DREAM: ${DATA_FINAL_PREFIX_DREAM}

    AREA_FILENAME: ${AREA_FILENAME}
    
    EP_FILENAME_SEJU: ${EP_FILENAME_SEJU}
    EP_FILENAME_DREAM: ${EP_FILENAME_DREAM}
    TITLE_MAP_SEJU: ${TITLE_MAP_SEJU}
    TITLE_MAP_DREAM: ${TITLE_MAP_DREAM}

    # SFTP
    SFTP_HOST: ${SFTP_HOST}
    SFTP_PORT: ${SFTP_PORT}
    SFTP_USER: ${SFTP_USER}
    SFTP_PASSWORD: ${SFTP_PASSWORD}
    SFTP_REMOTE_PATH: ${SFTP_REMOTE_PATH}
    SFTP_REMOTE_PATH_DREAM: ${SFTP_REMOTE_PATH_DREAM}

    # DAG
    SEJU_DATA_DAG_TIME: ${SEJU_DATA_DAG_TIME}

    # IDs
    SEJU_ID: ${SEJU_ID}
    DREAM_ID: ${DREAM_ID}

    # Remote Logging (S3)
    AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
    AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: s3://${LOG_BUCKET}/airflow-logs
    AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: aws_default
    AIRFLOW__LOGGING__LOG_RETENTION_DAYS: "14"
    
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    
  user: "${AIRFLOW_UID}:0"
  depends_on:
    postgres:
      condition: service_healthy
  restart: unless-stopped

services:
  postgres:
    image: postgres:15
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
    restart: unless-stopped

  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    entrypoint: /bin/bash
    command: >
      -c "
      airflow db migrate
      "
    restart: "no"

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $$(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5


volumes:
  postgres-data: